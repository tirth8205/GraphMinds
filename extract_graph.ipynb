{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "847944ad-e3cc-42d3-bdd9-2720887c0750",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # Importing the pandas library for data manipulation and analysis\n",
    "import numpy as np  # Importing the numpy library for numerical operations\n",
    "import os  # Importing the os library to interact with the operating system\n",
    "from langchain.document_loaders import PyPDFLoader, UnstructuredPDFLoader, PyPDFium2Loader  # Importing PDF loaders for processing PDFs\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader, DirectoryLoader  # Importing loaders to handle PDF directories\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # Importing a text splitter for breaking down documents\n",
    "from pathlib import Path  # Importing Path from pathlib for working with file paths\n",
    "import random  # Importing random library to use for generating random values\n",
    "\n",
    "## Input data directory\n",
    "inputdirectory = Path(\"./input\")  # Setting the input directory path\n",
    "## This is where the output csv files will be written\n",
    "outputdirectory = Path(\"./output\")  # Setting the output directory path for CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a75e54-87da-40d0-94e9-775931d51c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the PDF document\n",
    "loader = PyPDFLoader(\"input/#FileName\")  # Initialising the PDF loader with the specified file\n",
    "documents = loader.load()  # Loading the content of the PDF into a variable\n",
    "\n",
    "# Split the document into chunks\n",
    "splitter = RecursiveCharacterTextSplitter(  # Initialising the text splitter for chunking the document\n",
    "    chunk_size=1500,  # Defining the size of each chunk in characters\n",
    "    chunk_overlap=150,  # Defining the overlap between chunks\n",
    "    length_function=len,  # Using the length function to determine the size of each chunk\n",
    "    is_separator_regex=False,  # Setting whether the separator is a regex or not\n",
    ")\n",
    "\n",
    "pages = splitter.split_documents(documents)  # Splitting the document into chunks\n",
    "\n",
    "# Save the chunks to a text file\n",
    "with open(\"output/chunks.txt\", \"w\") as file:  # Opening a file to write the chunks to\n",
    "    for chunk in pages:  # Iterating over each chunk of the document\n",
    "        file.write(chunk.page_content + \"\\n\\n\")  # Writing each chunk and separating by two newlines\n",
    "\n",
    "print(\"Number of chunks = \", len(pages))  # Printing the number of chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b2f592-8430-4573-8a7d-9b12de9890fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.df_helpers import documents2Dataframe  # Importing a helper function to convert documents to a DataFrame\n",
    "df = documents2Dataframe(pages)  # Converting the split document pages into a DataFrame\n",
    "print(df.shape)  # Printing the shape (rows, columns) of the DataFrame\n",
    "df.head()  # Displaying the first few rows of the DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84bf7b08-87f3-4a11-a2da-2ba5ee69c046",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function uses the helpers/prompt function to extract concepts from text\n",
    "from helpers.df_helpers import df2Graph  # Importing the df2Graph function from df_helpers to generate graph data from a DataFrame\n",
    "from helpers.df_helpers import graph2Df  # Importing the graph2Df function from df_helpers to convert graph nodes into a DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e60ec7-14a3-4486-98d9-d7d878d01d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # Importing os to interact with the file system\n",
    "import pandas as pd  # Importing pandas for data manipulation\n",
    "import numpy as np  # Importing numpy for numerical operations\n",
    "\n",
    "# To regenerate the graph with LLM, set this to True\n",
    "regenerate = True  # Flag to control whether the graph is regenerated\n",
    "\n",
    "if regenerate:\n",
    "    # Extract concepts from the DataFrame using the specified model\n",
    "    concepts_list = df2Graph(df, model='zephyr:latest')  # Generating graph data from the DataFrame using the model 'zephyr:latest'\n",
    "    # Convert the list of concepts into a DataFrame\n",
    "    dfg1 = graph2Df(concepts_list)  # Converting the generated graph nodes into a DataFrame\n",
    "    \n",
    "    # Create the output directory if it doesn't exist\n",
    "    if not os.path.exists(outputdirectory):  # Checking if the output directory exists\n",
    "        os.makedirs(outputdirectory)  # Creating the directory if it doesn't exist\n",
    "    \n",
    "    # Save the generated graph DataFrame to a CSV file\n",
    "    dfg1.to_csv(os.path.join(outputdirectory, \"graph.csv\"), sep=\"|\", index=False)  # Saving the graph DataFrame to 'graph.csv'\n",
    "    # Save the original DataFrame chunks to a CSV file\n",
    "    df.to_csv(os.path.join(outputdirectory, \"chunks.csv\"), sep=\"|\", index=False)  # Saving the original DataFrame chunks to 'chunks.csv'\n",
    "else:\n",
    "    # Load the graph DataFrame from an existing CSV file\n",
    "    dfg1 = pd.read_csv(os.path.join(outputdirectory, \"graph.csv\"), sep=\"|\")  # Loading the graph DataFrame from 'graph.csv'\n",
    "\n",
    "# Replace empty strings with NaN\n",
    "dfg1.replace(\"\", np.nan, inplace=True)  # Replacing empty strings in the DataFrame with NaN\n",
    "# Drop rows with NaN values in 'node_1', 'node_2', or 'edge' columns\n",
    "dfg1.dropna(subset=[\"node_1\", \"node_2\", 'edge'], inplace=True)  # Dropping rows where 'node_1', 'node_2', or 'edge' is NaN\n",
    "# Set the initial count value to 4 for each row\n",
    "dfg1['count'] = 4  # Assigning a count value of 4 to all rows in the DataFrame\n",
    "\n",
    "# Print the shape of the cleaned DataFrame\n",
    "print(dfg1.shape)  # Printing the shape (rows, columns) of the cleaned DataFrame\n",
    "# Display the first few rows of the cleaned DataFrame\n",
    "dfg1.head()  # Displaying the first few rows of the cleaned DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a60566-9129-4c6d-8964-6b9dee8d0c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contextual_proximity(input_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Melt the dataframe into a long format with two columns for chunk_id and node\n",
    "    # This will turn node_1 and node_2 into rows with a single \"node\" column\n",
    "    melted_df = pd.melt(\n",
    "        input_df, id_vars=[\"chunk_id\"], value_vars=[\"node_1\", \"node_2\"], value_name=\"node\"\n",
    "    )\n",
    "    # Remove the \"variable\" column that was created during the melting process\n",
    "    melted_df.drop(columns=[\"variable\"], inplace=True)\n",
    "    \n",
    "    # Perform a self-join on the melted dataframe using \"chunk_id\" to link nodes from the same chunk\n",
    "    # This will create all possible combinations of terms within the same chunk\n",
    "    merged_df = pd.merge(melted_df, melted_df, on=\"chunk_id\", suffixes=(\"_1\", \"_2\"))\n",
    "    \n",
    "    # Identify and drop rows where the node in both columns is the same (self-loops)\n",
    "    self_loops_drop = merged_df[merged_df[\"node_1\"] == merged_df[\"node_2\"]].index\n",
    "    filtered_df = merged_df.drop(index=self_loops_drop).reset_index(drop=True)\n",
    "    \n",
    "    # Group by pairs of nodes and aggregate the chunk_id by joining them into a list and counting occurrences\n",
    "    grouped_df = (\n",
    "        filtered_df.groupby([\"node_1\", \"node_2\"])\n",
    "        .agg({\"chunk_id\": [\",\".join, \"count\"]})  # Join chunk_ids and count their frequency for each pair of nodes\n",
    "        .reset_index()\n",
    "    )\n",
    "    \n",
    "    # Rename the columns to make them clearer\n",
    "    grouped_df.columns = [\"node_1\", \"node_2\", \"chunk_id\", \"count\"]\n",
    "    \n",
    "    # Replace empty strings with NaN values (in case there are any)\n",
    "    grouped_df.replace(\"\", np.nan, inplace=True)\n",
    "    \n",
    "    # Drop rows where either node_1 or node_2 is missing (NaN values)\n",
    "    grouped_df.dropna(subset=[\"node_1\", \"node_2\"], inplace=True)\n",
    "    \n",
    "    # Drop rows where the count of the node pair is only 1 (keep only pairs that appear more than once)\n",
    "    grouped_df = grouped_df[grouped_df[\"count\"] != 1]\n",
    "    \n",
    "    # Add a column to indicate that this edge represents \"contextual proximity\"\n",
    "    grouped_df[\"edge\"] = \"contextual proximity\"\n",
    "    \n",
    "    # Return the processed dataframe\n",
    "    return grouped_df\n",
    "\n",
    "\n",
    "# Call the function to generate the new dataframe based on the provided input_df\n",
    "output_df = contextual_proximity(dfg1)\n",
    "\n",
    "# Display the last few rows of the resulting dataframe\n",
    "output_df.tail()\n",
    "\n",
    "# Print the original dataframe (dfg1) for reference\n",
    "print(dfg1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcdf5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # Importing pandas for data manipulation\n",
    "import numpy as np  # Importing numpy for numerical operations\n",
    "\n",
    "def contextual_proximity(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Melt the dataframe into a list of nodes\n",
    "    dfg_long = pd.melt(\n",
    "        df, id_vars=[\"chunk_id\"], value_vars=[\"node_1\", \"node_2\"], value_name=\"node\"\n",
    "    )  # Convert the 'node_1' and 'node_2' columns into a single 'node' column, keeping 'chunk_id' as the identifier\n",
    "    dfg_long.drop(columns=[\"variable\"], inplace=True)  # Drop the unnecessary 'variable' column created by the melt function\n",
    "    \n",
    "    # Self join with chunk_id as the key will create a link between terms occurring in the same text chunk\n",
    "    dfg_wide = pd.merge(dfg_long, dfg_long, on=\"chunk_id\", suffixes=(\"_1\", \"_2\"))  # Perform a self-join on 'chunk_id' to link nodes that occur together in the same chunk\n",
    "    \n",
    "    # Drop self loops (i.e., rows where node_1 is the same as node_2)\n",
    "    self_loops_drop = dfg_wide[dfg_wide[\"node_1\"] == dfg_wide[\"node_2\"]].index  # Identify rows where 'node_1' and 'node_2' are the same\n",
    "    dfg2 = dfg_wide.drop(index=self_loops_drop).reset_index(drop=True)  # Remove self-loops and reset the DataFrame's index\n",
    "    \n",
    "    # Group and count direct edges between node_1 and node_2\n",
    "    dfg2 = (\n",
    "        dfg2.groupby([\"node_1\", \"node_2\"])  # Group by the node pairs\n",
    "        .agg({\"chunk_id\": [\",\".join, \"count\"]})  # Aggregate the chunk_id as a comma-separated string and count occurrences\n",
    "        .reset_index()  # Reset the index\n",
    "    )\n",
    "    dfg2.columns = [\"node_1\", \"node_2\", \"chunk_id\", \"count\"]  # Rename the columns\n",
    "    dfg2.replace(\"\", np.nan, inplace=True)  # Replace empty strings with NaN\n",
    "    dfg2.dropna(subset=[\"node_1\", \"node_2\"], inplace=True)  # Drop rows where either node_1 or node_2 is NaN\n",
    "    \n",
    "    # Drop edges that occur only once (optional, based on use case)\n",
    "    dfg2 = dfg2[dfg2[\"count\"] != 1]  # Remove edges that only appear once, as they might not be significant\n",
    "    dfg2[\"edge\"] = \"contextual proximity\"  # Label these edges as \"contextual proximity\"\n",
    "    \n",
    "    # Create a set of indirect edges based on shared intermediate nodes\n",
    "    indirect_edges = []  # Initialize a list to store indirect edges\n",
    "    nodes = dfg2[['node_1', 'node_2']].stack().unique()  # Get a unique list of all nodes\n",
    "\n",
    "    for node in nodes:\n",
    "        # Get all nodes directly connected to the current node\n",
    "        connected_nodes = pd.concat([\n",
    "            dfg2[dfg2['node_1'] == node][['node_2', 'chunk_id']],  # Find nodes where 'node_1' is the current node\n",
    "            dfg2[dfg2['node_2'] == node][['node_1', 'chunk_id']].rename(columns={'node_1': 'node_2'})  # Find nodes where 'node_2' is the current node and rename to 'node_2'\n",
    "        ])\n",
    "        \n",
    "        # Create pairs of these connected nodes to form indirect edges\n",
    "        for i in range(len(connected_nodes)):\n",
    "            for j in range(i + 1, len(connected_nodes)):\n",
    "                pair = sorted([connected_nodes.iloc[i]['node_2'], connected_nodes.iloc[j]['node_2']])  # Sort the node pairs to avoid duplicate combinations\n",
    "                chunk_ids = ','.join([connected_nodes.iloc[i]['chunk_id'], connected_nodes.iloc[j]['chunk_id']])  # Concatenate chunk_ids\n",
    "\n",
    "                # Append the indirect edge with the intermediate node\n",
    "                indirect_edges.append((pair[0], pair[1], node, chunk_ids))\n",
    "\n",
    "    # Convert indirect edges into a DataFrame\n",
    "    indirect_df = pd.DataFrame(indirect_edges, columns=[\"node_1\", \"node_2\", \"via_node\", \"chunk_id\"])\n",
    "    \n",
    "    # Group the indirect edges by node pairs and count how many times they occur\n",
    "    indirect_df = (\n",
    "        indirect_df.groupby([\"node_1\", \"node_2\"])  # Group by the node pairs\n",
    "        .agg({\"chunk_id\": \",\".join, \"via_node\": \"count\"})  # Aggregate chunk_ids and count how many times the indirect edge appears\n",
    "        .reset_index()  # Reset the index\n",
    "    )\n",
    "    indirect_df.columns = [\"node_1\", \"node_2\", \"chunk_id\", \"count\"]  # Rename columns\n",
    "    indirect_df[\"edge\"] = \"indirect contextual proximity\"  # Label these edges as \"indirect contextual proximity\"\n",
    "    \n",
    "    # Merge indirect edges with the direct edges\n",
    "    final_df = pd.concat([dfg2, indirect_df], ignore_index=True)  # Concatenate the direct and indirect edges\n",
    "\n",
    "    # Handle cases where direct and indirect edges overlap\n",
    "    final_df = (\n",
    "        final_df.groupby([\"node_1\", \"node_2\", \"edge\"])  # Group by node pairs and edge type\n",
    "        .agg({\"chunk_id\": \",\".join, \"count\": \"sum\"})  # Aggregate the chunk_ids and sum the count for overlapping edges\n",
    "        .reset_index()  # Reset the index\n",
    "    )\n",
    "    \n",
    "    return final_df  # Return the final DataFrame containing both direct and indirect edges\n",
    "\n",
    "# Usage: apply the contextual proximity function to a DataFrame containing graph data\n",
    "dfg2 = contextual_proximity(dfg1)\n",
    "\n",
    "# Display the last few rows of the resulting DataFrame\n",
    "dfg2.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c308096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the final DataFrame (dfg2) which contains both direct and indirect edges\n",
    "print(dfg2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da6c4cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the two DataFrames (dfg1 and dfg2) along the rows (axis=0)\n",
    "# This combines both the direct edges (dfg1) and the newly generated edges (dfg2)\n",
    "dfg = pd.concat([dfg1, dfg2], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecc38c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Group and aggregate the DataFrame by 'node_1' and 'node_2', concatenating 'chunk_id' and 'edge' as comma-separated strings\n",
    "# and summing the 'count' for the same node pairs, then reset the index\n",
    "updated_dfg = (\n",
    "    dfg.groupby([\"node_1\", \"node_2\"])\n",
    "    .agg({\"chunk_id\": \",\".join, \"edge\": ','.join, 'count': 'sum'})  # Aggregating chunk_id and edge, summing the count\n",
    "    .reset_index()  # Reset the index to flatten the grouped DataFrame\n",
    ")\n",
    "\n",
    "# Step 2: Print the resulting DataFrame (optional, for checking the output)\n",
    "print(updated_dfg)\n",
    "\n",
    "# Step 3: Define the output directory and file path\n",
    "output_directory = \"output\"  # Directory where the file will be saved\n",
    "output_file = os.path.join(output_directory, \"updated_dfg_grouped.csv\")  # Full path for the output CSV file\n",
    "\n",
    "# Save the updated DataFrame to the CSV file\n",
    "updated_dfg.to_csv(output_file, index=False)  # Save the DataFrame without writing row indices\n",
    "\n",
    "# Print a message to confirm that the file has been saved\n",
    "print(f\"DataFrame saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0e5629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate_chunk_ids_and_save(updated_dfg: pd.DataFrame, output_file: str):\n",
    "    # Iterate over each row in the DataFrame and process the 'chunk_id' column\n",
    "    # Split the 'chunk_id' string by commas, remove duplicates using set, sort the values, and join them back into a comma-separated string\n",
    "    updated_dfg['chunk_id'] = updated_dfg['chunk_id'].apply(lambda x: ','.join(sorted(set(x.split(',')))))\n",
    "\n",
    "    # Save the updated DataFrame back to the specified file\n",
    "    updated_dfg.to_csv(output_file, index=False)  # Save the DataFrame without row indices\n",
    "    print(f\"DataFrame with unique chunk_ids saved to {output_file}\")  # Print confirmation message\n",
    "\n",
    "# Example usage after the grouping and aggregation:\n",
    "output_directory = \"output\"  # Define the output directory\n",
    "output_file = os.path.join(output_directory, \"updated_dfg_grouped.csv\")  # Define the output file path\n",
    "\n",
    "# Call the function to remove duplicate chunk_ids and save the file\n",
    "remove_duplicate_chunk_ids_and_save(updated_dfg, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6053c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # Importing pandas for data manipulation\n",
    "import os  # Importing os to interact with the file system\n",
    "\n",
    "# Define the path to the chunks.csv file\n",
    "chunks_file = os.path.join(\"output\", \"chunks.csv\")  # Setting the path to the chunks.csv file located in the 'output' directory\n",
    "\n",
    "# Load the chunks.csv file into a DataFrame using the correct delimiter\n",
    "chunks_df = pd.read_csv(chunks_file, delimiter='|')  # Reading the CSV file with '|' as the delimiter and loading it into a DataFrame\n",
    "\n",
    "# Display the first few rows of the DataFrame to confirm it's loaded correctly\n",
    "print(chunks_df.head())  # Printing the first few rows of the DataFrame to verify the contents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb66dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # Importing pandas for data manipulation\n",
    "\n",
    "# Function to replace chunk_ids in updated_dfg with the corresponding content from chunks_df\n",
    "def replace_chunk_ids_with_content(updated_dfg: pd.DataFrame, chunks_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Create a dictionary to map chunk_id to its corresponding text content\n",
    "    chunk_id_to_content = chunks_df.set_index('chunk_id')['text'].to_dict()  # Setting 'chunk_id' as the index and mapping it to 'text'\n",
    "\n",
    "    # Function to replace chunk_ids in a row with the corresponding text content\n",
    "    def replace_chunk_id_with_text(chunk_ids):\n",
    "        # Split the chunk_ids by commas, remove duplicates, and replace each chunk_id with its associated text content\n",
    "        unique_chunk_ids = set(chunk_ids.split(','))  # Remove duplicates by using a set\n",
    "        # For each chunk_id, get its corresponding content from the dictionary, or show an error message if not found\n",
    "        replaced_content = [chunk_id_to_content.get(chunk_id.strip(), f\"[Unknown chunk_id: {chunk_id}]\") for chunk_id in unique_chunk_ids]\n",
    "        return ' '.join(replaced_content)  # Join the replaced content into a single string\n",
    "\n",
    "    # Create a copy of updated_dfg to store the new content (so the original DataFrame remains unchanged)\n",
    "    contentreplacedforchunk_dfg = updated_dfg.copy()\n",
    "    \n",
    "    # Apply the function to the 'chunk_id' column of the new DataFrame\n",
    "    contentreplacedforchunk_dfg['chunk_id'] = contentreplacedforchunk_dfg['chunk_id'].apply(replace_chunk_id_with_text)  # Replacing chunk_ids with text content\n",
    "\n",
    "    # Rename the 'chunk_id' column to 'text_from_chunk_id' to reflect the new content\n",
    "    contentreplacedforchunk_dfg.rename(columns={'chunk_id': 'text_from_chunk_id'}, inplace=True)\n",
    "    \n",
    "    # Return the new DataFrame with the chunk content replaced\n",
    "    return contentreplacedforchunk_dfg\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "# Replace chunk_ids with content and get the new DataFrame\n",
    "contentreplacedforchunk_dfg = replace_chunk_ids_with_content(updated_dfg, chunks_df)\n",
    "\n",
    "# Save the new DataFrame to a CSV file\n",
    "output_file = os.path.join(\"output\", \"contentreplacedforchunk_dfg.csv\")  # Define the output file path\n",
    "contentreplacedforchunk_dfg.to_csv(output_file, index=False)  # Save the DataFrame without row indices\n",
    "\n",
    "# Print a message to confirm that the new DataFrame has been saved\n",
    "print(f\"DataFrame with chunk content saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10751907",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # Importing pandas for data manipulation\n",
    "import os  # Importing os to interact with the file system\n",
    "from ollama.client import generate  # Importing the generate function from the Ollama client to interact with the LLM\n",
    "\n",
    "# Function to update edge labels using an LLM\n",
    "def update_edge_labels_with_llm(contentreplacedforchunk_dfg: pd.DataFrame, model_name: str) -> pd.DataFrame:\n",
    "    # Function to interact with the LLM and get the refined edge label\n",
    "    def get_refined_edge(node_1, node_2, text_from_chunk_id):\n",
    "        # Creating the prompt for the LLM\n",
    "        prompt = (\n",
    "            f\"Based on the following context, please provide a short and accurate label \"\n",
    "            f\"for the relationship between \\\"{node_1}\\\" and \\\"{node_2}\\\". \"\n",
    "            f\"The label should be concise and meaningful. Do not use any outside knowledge, only use the context provided.\\n\\n\"\n",
    "            f\"Context: \\\"{text_from_chunk_id}\\\"\"\n",
    "        )\n",
    "        \n",
    "        # Call the LLM model using the Ollama client\n",
    "        response, _ = generate(model_name=model_name, prompt=prompt)  # Unpack the response from the LLM call\n",
    "        refined_edge = response.strip()  # Strip any extra whitespace from the response\n",
    "        \n",
    "        # If the response is empty or unhelpful, default to \"contextual proximity\"\n",
    "        if not refined_edge or refined_edge.lower() in [\"\", \"unknown\", \"not found\"]:\n",
    "            refined_edge = \"contextual proximity\"  # Default label if no useful response is given\n",
    "        \n",
    "        return refined_edge  # Return the refined edge label\n",
    "    \n",
    "    # Iterate over each row in the DataFrame and update the edge label using the LLM\n",
    "    for index, row in contentreplacedforchunk_dfg.iterrows():\n",
    "        node_1 = row['node_1']  # Extract the first node from the row\n",
    "        node_2 = row['node_2']  # Extract the second node from the row\n",
    "        text_from_chunk_id = row['text_from_chunk_id']  # Extract the context text from the chunk_id\n",
    "        \n",
    "        # Get the refined edge label from the LLM based on the nodes and their context\n",
    "        refined_edge = get_refined_edge(node_1, node_2, text_from_chunk_id)\n",
    "        \n",
    "        # Update the edge label in the DataFrame at the current row\n",
    "        contentreplacedforchunk_dfg.at[index, 'edge'] = refined_edge\n",
    "    \n",
    "    # Save the updated DataFrame to a CSV file\n",
    "    output_file = os.path.join(\"output\", \"contentreplacedforchunk_dfg.csv\")  # Define the output file path\n",
    "    contentreplacedforchunk_dfg.to_csv(output_file, index=False)  # Save the DataFrame without row indices\n",
    "    \n",
    "    print(f\"Updated DataFrame with refined edge labels saved to {output_file}\")  # Confirm that the file has been saved\n",
    "    \n",
    "    return contentreplacedforchunk_dfg  # Return the updated DataFrame\n",
    "\n",
    "# Function to check if the file already exists and process it accordingly\n",
    "def check_and_process_file(contentreplacedforchunk_dfg: pd.DataFrame, model_name: str):\n",
    "    output_file = os.path.join(\"output\", \"contentreplacedforchunk_dfg.csv\")  # Define the output file path\n",
    "    \n",
    "    # Check if the file already exists\n",
    "    if os.path.exists(output_file):\n",
    "        # Prompt the user to choose whether to regenerate the file\n",
    "        user_input = input(f\"The file {output_file} already exists. Do you want to regenerate it? (yes/no): \").strip().lower()\n",
    "        \n",
    "        if user_input == 'no':\n",
    "            # If the user chooses not to regenerate, use the existing file\n",
    "            print(f\"Using the existing file: {output_file}\")\n",
    "            return pd.read_csv(output_file)  # Load and return the existing file as a DataFrame\n",
    "        elif user_input == 'yes':\n",
    "            # If the user chooses to regenerate, proceed with the regeneration\n",
    "            print(\"Regenerating the file...\")\n",
    "        else:\n",
    "            # If the input is invalid, ask again\n",
    "            print(\"Invalid input. Please enter 'yes' or 'no'.\")\n",
    "            return check_and_process_file(contentreplacedforchunk_dfg, model_name)  # Recursively ask for valid input\n",
    "    \n",
    "    # If the file doesn't exist or the user chose to regenerate it, update the edge labels with the LLM\n",
    "    return update_edge_labels_with_llm(contentreplacedforchunk_dfg, model_name)\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "# Assuming contentreplacedforchunk_dfg is already defined and contains the DataFrame\n",
    "\n",
    "# Check if the file exists and process it accordingly\n",
    "contentreplacedforchunk_dfg = check_and_process_file(contentreplacedforchunk_dfg, model_name=\"mistral-openorca:latest\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b388607d-0255-4699-9d5e-39887a0ae9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the 'node_1' column from contentreplacedforchunk_dfg and 'node_2' column from dfg along the rows (axis=0)\n",
    "nodes = pd.concat([contentreplacedforchunk_dfg['node_1'], dfg['node_2']], axis=0).unique()\n",
    "\n",
    "# Output the shape (i.e., number of unique nodes) from the concatenated result\n",
    "nodes.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "963874ae-c362-425f-baf4-644620f62ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx  # Importing networkx for graph creation and manipulation\n",
    "\n",
    "# Initialize a new graph\n",
    "G = nx.Graph()\n",
    "\n",
    "## Add nodes to the graph\n",
    "for node in nodes:\n",
    "    G.add_node(\n",
    "        str(node)  # Adding each node to the graph, converting the node to a string for consistency\n",
    "    )\n",
    "\n",
    "## Add edges to the graph\n",
    "for index, row in contentreplacedforchunk_dfg.iterrows():\n",
    "    G.add_edge(\n",
    "        str(row[\"node_1\"]),  # Adding an edge between node_1 and node_2\n",
    "        str(row[\"node_2\"]),  # Convert both nodes to strings for consistency\n",
    "        title=row[\"edge\"],  # Using the 'edge' label from the DataFrame for a meaningful relationship label\n",
    "        weight=row['count']/4  # Assigning a weight to the edge based on the 'count' divided by 4 for scaling\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f3b3e8-3dc3-43ed-8404-86a13aabcc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate communities using the Girvan-Newman algorithm\n",
    "communities_generator = nx.community.girvan_newman(G)  # Create a generator for communities using the Girvan-Newman algorithm\n",
    "\n",
    "# Get the top level of communities (first split)\n",
    "top_level_communities = next(communities_generator)  # Get the first level of communities from the generator\n",
    "\n",
    "# Get the next level of communities (further refined split)\n",
    "next_level_communities = next(communities_generator)  # Get the next level of communities (more detailed split)\n",
    "\n",
    "# Sort and organize the communities into a list of sorted node groups\n",
    "communities = sorted(map(sorted, next_level_communities))  # Sort the nodes within each community and the list of communities\n",
    "\n",
    "# Print the number of communities detected\n",
    "print(\"Number of Communities = \", len(communities))  # Output the total number of communities\n",
    "\n",
    "# Print the communities\n",
    "print(communities)  # Output the list of communities with the nodes belonging to each\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479b2fa1-fd0e-4a58-a1fb-9e4e5767b514",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns  # Importing seaborn to generate color palettes\n",
    "palette = \"hls\"  # Defining the color palette to be used (hue-lightness-saturation)\n",
    "\n",
    "## Function to assign colors to communities and create a new DataFrame\n",
    "def colors2Community(communities) -> pd.DataFrame:\n",
    "    ## Define a color palette with as many colors as there are communities\n",
    "    p = sns.color_palette(palette, len(communities)).as_hex()  # Generate a list of hex color codes for each community\n",
    "    \n",
    "    random.shuffle(p)  # Shuffle the color palette to randomize the color assignments\n",
    "\n",
    "    rows = []  # Initialize an empty list to store node-color-group mappings\n",
    "    group = 0  # Group identifier for each community\n",
    "    \n",
    "    # Iterate over each community and assign colors\n",
    "    for community in communities:\n",
    "        color = p.pop()  # Pop a color from the shuffled palette\n",
    "        group += 1  # Increment the group counter for each new community\n",
    "        \n",
    "        # Assign each node in the community a color and group\n",
    "        for node in community:\n",
    "            rows += [{\"node\": node, \"color\": color, \"group\": group}]  # Append node-color-group mapping to the list\n",
    "\n",
    "    # Convert the list of mappings into a DataFrame\n",
    "    df_colors = pd.DataFrame(rows)  # Create a DataFrame with 'node', 'color', and 'group' columns\n",
    "\n",
    "    return df_colors  # Return the DataFrame with nodes, colors, and group assignments\n",
    "\n",
    "# Call the function to create the colors DataFrame for the communities\n",
    "colors = colors2Community(communities)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "colors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "57c37866-ad09-4945-8031-d942d36da2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through each row of the colors DataFrame and assign attributes to the nodes in the graph G\n",
    "for index, row in colors.iterrows():\n",
    "    G.nodes[row['node']]['group'] = row['group']  # Assign the 'group' attribute to the node\n",
    "    G.nodes[row['node']]['color'] = row['color']  # Assign the 'color' attribute to the node\n",
    "    G.nodes[row['node']]['size'] = G.degree[row['node']]  # Assign the 'size' attribute based on the node's degree (number of connections)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4136f7cd-a4ae-4ccb-954e-2ea5ae4f3ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvis.network import Network  # Importing the Network class from the pyvis library for network visualization\n",
    "import os  # Importing os for file system interactions\n",
    "\n",
    "# Define the output directory and file path for saving the HTML visualization\n",
    "output_directory = \"./docs\"  # Directory where the HTML file will be saved\n",
    "graph_output_directory = os.path.join(output_directory, \"index.html\")  # Full path to the output HTML file\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)  # Ensure that the output directory exists, or create it\n",
    "\n",
    "# Initialize a Pyvis Network object for visualization\n",
    "net = Network(\n",
    "    notebook=False,  # Disable notebook mode (for saving as an HTML file)\n",
    "    cdn_resources=\"remote\",  # Use remote CDN resources for the visualization\n",
    "    height=\"900px\",  # Set the height of the visualization\n",
    "    width=\"100%\",  # Set the width of the visualization to take up 100% of the available width\n",
    "    select_menu=True,  # Enable a select menu for node selection\n",
    "    filter_menu=False,  # Disable the filter menu\n",
    ")\n",
    "\n",
    "# Add the nodes and edges from the NetworkX graph (G) to the Pyvis network object\n",
    "net.from_nx(G)\n",
    "\n",
    "# Apply the Force Atlas 2 algorithm for graph layout\n",
    "net.force_atlas_2based(central_gravity=0.015, gravity=-31)  # Apply force-atlas layout settings\n",
    "\n",
    "# Optional: Show buttons to control physics settings\n",
    "net.show_buttons(filter_=[\"physics\"])  # Add buttons to enable users to adjust the physics parameters in the visualization\n",
    "\n",
    "# Save the graph visualization as an HTML file\n",
    "net.show(graph_output_directory)  # Save and display the graph in the specified output directory as \"index.html\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f466d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer  # Importing SentenceTransformer for generating embeddings\n",
    "from scipy.spatial.distance import cosine  # Importing cosine similarity calculation\n",
    "import pandas as pd  # Importing pandas for data manipulation\n",
    "from ollama.client import generate  # Importing Ollama client for LLM generation\n",
    "\n",
    "# Load the pre-trained model for generating embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Using a pre-trained model for text embeddings\n",
    "\n",
    "def generate_embeddings(text_list):\n",
    "    \"\"\"Generate embeddings for a list of texts using a pre-trained SentenceTransformer model.\"\"\"\n",
    "    return model.encode(text_list)  # Generate and return embeddings for the provided list of texts\n",
    "\n",
    "def add_embeddings_to_contentreplacedforchunk_dfg(contentreplacedforchunk_dfg: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add embeddings to the DataFrame `contentreplacedforchunk_dfg` based on combined text from nodes and edges.\"\"\"\n",
    "    # Combine node_1, edge, and node_2 into a single text string\n",
    "    contentreplacedforchunk_dfg['combined_text'] = contentreplacedforchunk_dfg.apply(lambda row: f\"{row['node_1']} {row['edge']} {row['node_2']}\", axis=1)\n",
    "    \n",
    "    # Generate embeddings for the combined text\n",
    "    embeddings = generate_embeddings(contentreplacedforchunk_dfg['combined_text'].tolist())\n",
    "    \n",
    "    # Store embeddings in the DataFrame\n",
    "    contentreplacedforchunk_dfg['embedding'] = list(embeddings)  # Add embeddings as a new column in the DataFrame\n",
    "    \n",
    "    return contentreplacedforchunk_dfg  # Return the DataFrame with embeddings\n",
    "\n",
    "def generate_final_answer_with_llm(relationships, nodes_entities, context):\n",
    "    \"\"\"Generate the final answer using the LLM.\"\"\"\n",
    "    # Create a prompt to pass to the LLM\n",
    "    prompt = (\n",
    "        f\"Given the following relationships and context, provide a summary answer to the query:\\n\\n\"\n",
    "        f\"Relationships:\\n{relationships}\\n\\n\"\n",
    "        f\"Entities Involved:\\n{nodes_entities}\\n\\n\"\n",
    "        f\"Context:\\n{context}\\n\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "    \n",
    "    # Call the LLM model to generate the response based on the prompt\n",
    "    full_response, _ = generate(\"mistral-openorca:latest\", prompt)  # Using the LLM with the prompt\n",
    "    return full_response.strip()  # Return the response after stripping any extra whitespace\n",
    "\n",
    "def answer_query_with_all_relationships(query: str, contentreplacedforchunk_dfg: pd.DataFrame, df: pd.DataFrame, similarity_threshold=0.3) -> str:\n",
    "    \"\"\"Answer a user query by gathering all relevant relationships and generating a final answer with LLM.\"\"\"\n",
    "    # Step 1: Generate an embedding for the user query\n",
    "    query_embedding = generate_embeddings([query])[0]  # Generate embedding for the query text\n",
    "    \n",
    "    # Step 2: Compute cosine similarity between the query embedding and embeddings in contentreplacedforchunk_dfg\n",
    "    contentreplacedforchunk_dfg['similarity'] = contentreplacedforchunk_dfg['embedding'].apply(lambda emb: 1 - cosine(query_embedding, emb))\n",
    "    \n",
    "    # Step 3: Filter rows based on a similarity threshold\n",
    "    relevant_rows = contentreplacedforchunk_dfg[contentreplacedforchunk_dfg['similarity'] >= similarity_threshold]  # Select rows with high similarity\n",
    "    \n",
    "    # Step 4: Gather all relevant relationships and contexts\n",
    "    relationships = []\n",
    "    context_list = []\n",
    "    nodes_entities = set()  # To collect unique nodes and entities\n",
    "    \n",
    "    for _, row in relevant_rows.iterrows():\n",
    "        relationship = f\"{row['node_1']} - {row['edge']} - {row['node_2']}\"  # Create a string representing the relationship\n",
    "        relationships.append(relationship)  # Add the relationship to the list\n",
    "        context_list.append(get_context_from_chunks(row['text_from_chunk_id'].split(','), df))  # Get the context for the relationship\n",
    "        nodes_entities.update([row['node_1'], row['node_2']])  # Add unique nodes to the set\n",
    "    \n",
    "    relationships_text = \"\\n\".join(relationships)  # Combine all relationships into a single string\n",
    "    context = \" \".join(context_list)  # Combine all context into a single string\n",
    "    nodes_entities_text = \", \".join(nodes_entities)  # Combine all entities into a single string\n",
    "    \n",
    "    # Step 5: Generate the final answer using the LLM\n",
    "    final_answer = generate_final_answer_with_llm(relationships_text, nodes_entities_text, context)\n",
    "    \n",
    "    # Combine everything for the final output\n",
    "    answer = (\n",
    "        f\"Final Answer: {final_answer}\\n\\n\"\n",
    "        f\"All Relationships:\\n{relationships_text}\\n\\n\"\n",
    "        f\"Entities Involved:\\n{nodes_entities_text}\\n\\n\"\n",
    "        f\"Context:\\n{context}\"\n",
    "    )\n",
    "    \n",
    "    return answer  # Return the final answer\n",
    "\n",
    "def get_context_from_chunks(text_from_chunk_ids, df):\n",
    "    \"\"\"Retrieve context from the text chunks based on text_from_chunk_ids.\"\"\"\n",
    "    relevant_texts = df[df['chunk_id'].isin(text_from_chunk_ids)]['text'].tolist()  # Get text for the matching chunk_ids\n",
    "    return \" \".join(relevant_texts) if relevant_texts else \"\"  # Combine the text and return\n",
    "\n",
    "# Ensure `contentreplacedforchunk_dfg` is populated with your graph data, and `df` contains the relevant text chunks.\n",
    "contentreplacedforchunk_dfg = add_embeddings_to_contentreplacedforchunk_dfg(contentreplacedforchunk_dfg)  # Generate and add embeddings to the DataFrame\n",
    "\n",
    "# To answer a query:\n",
    "query = \"#ENTER YOUR QUERY HERE\"  # Example query\n",
    "response = answer_query_with_all_relationships(query, contentreplacedforchunk_dfg, df)\n",
    "\n",
    "# Print the response\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002a66b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer  # Importing SentenceTransformer for generating text embeddings\n",
    "from scipy.spatial.distance import cosine  # Importing cosine similarity calculation\n",
    "import pandas as pd  # Importing pandas for data manipulation\n",
    "from ollama.client import generate  # Importing Ollama client for interacting with the LLM\n",
    "\n",
    "# Load the pre-trained model for generating embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Using the 'all-MiniLM-L6-v2' model to generate embeddings\n",
    "\n",
    "def generate_embeddings(text_list):\n",
    "    \"\"\"Generate embeddings for a list of texts using a pre-trained SentenceTransformer model.\"\"\"\n",
    "    return model.encode(text_list)  # Generate and return embeddings for the provided text list\n",
    "\n",
    "def add_embeddings_to_dfg(dfg: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add embeddings to the DataFrame `dfg` based on combined text from nodes and edges.\"\"\"\n",
    "    # Combine node_1, edge, and node_2 into a single text string for each row\n",
    "    dfg['combined_text'] = dfg.apply(lambda row: f\"{row['node_1']} {row['edge']} {row['node_2']}\", axis=1)\n",
    "    \n",
    "    # Generate embeddings for the combined text\n",
    "    embeddings = generate_embeddings(dfg['combined_text'].tolist())\n",
    "    \n",
    "    # Store embeddings in the DataFrame\n",
    "    dfg['embedding'] = list(embeddings)  # Add the embeddings as a new column to the DataFrame\n",
    "    \n",
    "    return dfg  # Return the updated DataFrame\n",
    "\n",
    "def generate_final_answer_with_llm(relationships, nodes_entities, context):\n",
    "    \"\"\"Generate the final answer using the LLM.\"\"\"\n",
    "    # Create a prompt to pass to the LLM for generating a summary answer\n",
    "    prompt = (\n",
    "        f\"Given the following relationships and context, provide a summary answer to the query:\\n\\n\"\n",
    "        f\"Relationships:\\n{relationships}\\n\\n\"\n",
    "        f\"Entities Involved:\\n{nodes_entities}\\n\\n\"\n",
    "        f\"Context:\\n{context}\\n\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "    \n",
    "    # Generate a response using the LLM based on the prompt\n",
    "    full_response, _ = generate(\"mistral-openorca:latest\", prompt)  # Call the LLM to generate a response\n",
    "    return full_response.strip()  # Return the response after stripping extra whitespace\n",
    "\n",
    "def answer_query_with_all_relationships(query: str, dfg: pd.DataFrame, df: pd.DataFrame, similarity_threshold=0.5) -> str:\n",
    "    \"\"\"Answer a user query by gathering all relevant relationships and generating a final answer with LLM.\"\"\"\n",
    "    # Step 1: Generate an embedding for the user query\n",
    "    query_embedding = generate_embeddings([query])[0]  # Generate embedding for the query text\n",
    "    \n",
    "    # Step 2: Compute cosine similarity between the query embedding and embeddings in dfg\n",
    "    dfg['similarity'] = dfg['embedding'].apply(lambda emb: 1 - cosine(query_embedding, emb))  # Calculate cosine similarity\n",
    "    \n",
    "    # Step 3: Filter rows based on a similarity threshold\n",
    "    relevant_rows = dfg[dfg['similarity'] >= similarity_threshold]  # Select rows where similarity is above the threshold\n",
    "    \n",
    "    # Step 4: Gather all relevant relationships and contexts\n",
    "    relationships = []\n",
    "    context_list = []\n",
    "    nodes_entities = set()  # To collect unique nodes and entities\n",
    "    \n",
    "    for _, row in relevant_rows.iterrows():\n",
    "        relationship = f\"{row['node_1']} - {row['edge']} - {row['node_2']}\"  # Create a string representing the relationship\n",
    "        relationships.append(relationship)  # Append the relationship to the list\n",
    "        context_list.append(get_context_from_chunks(row['chunk_id'].split(','), df))  # Get the context for the relationship\n",
    "        nodes_entities.update([row['node_1'], row['node_2']])  # Add unique nodes to the set\n",
    "    \n",
    "    # Combine the relationships, context, and entities for the LLM input\n",
    "    relationships_text = \"\\n\".join(relationships)\n",
    "    context = \" \".join(context_list)\n",
    "    nodes_entities_text = \", \".join(nodes_entities)\n",
    "    \n",
    "    # Step 5: Generate the final answer using the LLM\n",
    "    final_answer = generate_final_answer_with_llm(relationships_text, nodes_entities_text, context)\n",
    "    \n",
    "    # Combine everything for the final output\n",
    "    answer = (\n",
    "        f\"Final Answer: {final_answer}\\n\\n\"\n",
    "        f\"All Relationships:\\n{relationships_text}\\n\\n\"\n",
    "        f\"Entities Involved:\\n{nodes_entities_text}\\n\\n\"\n",
    "        f\"Context:\\n{context}\"\n",
    "    )\n",
    "    \n",
    "    return answer  # Return the final answer\n",
    "\n",
    "def get_context_from_chunks(chunk_ids, df):\n",
    "    \"\"\"Retrieve context from the text chunks based on chunk IDs.\"\"\"\n",
    "    # Select the relevant text from `df` where 'chunk_id' matches the provided chunk IDs\n",
    "    relevant_texts = df[df['chunk_id'].isin(chunk_ids)]['text'].tolist()\n",
    "    return \" \".join(relevant_texts) if relevant_texts else \"\"  # Return combined text, or an empty string if no match\n",
    "\n",
    "def interactive_pdf_query(dfg: pd.DataFrame, df: pd.DataFrame):\n",
    "    \"\"\"Interactive chat function for querying the PDF.\"\"\"\n",
    "    print(\"You can now interact with the PDF. Ask questions about the document.\")\n",
    "    print(\"Type 'exit' or 'quit' to end the interaction.\")\n",
    "    \n",
    "    while True:\n",
    "        # Get the user's query\n",
    "        query = input(\"Ask your question: \").strip()\n",
    "        \n",
    "        # Check if the user wants to exit\n",
    "        if query.lower() in ['exit', 'quit']:\n",
    "            print(\"Ending the interaction. Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        # Answer the query using the function defined earlier\n",
    "        response = answer_query_with_all_relationships(query, dfg, df)\n",
    "        \n",
    "        # Display the response\n",
    "        print(\"\\n\" + response + \"\\n\")\n",
    "\n",
    "# Ensure `dfg` is populated with your graph data, and `df` contains the relevant text chunks.\n",
    "dfg = add_embeddings_to_dfg(dfg)  # Generate and add embeddings to the DataFrame\n",
    "\n",
    "# Start the interactive query session\n",
    "interactive_pdf_query(dfg, df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "knowledge-graph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
